import os
import sys
from openai import AzureOpenAI
from dotenv import load_dotenv
import asyncio
from typing import Generator

# Asegurar que encuentra los utils
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from utils import base_utils as bu
from utils import retrieval_utils as ru
from app.config import PROJECT_ROOT, DATA_DIR, CONFIGS_DIR

load_dotenv()

_AZURE_CLIENT = None
_RAG_COMPONENTS = None

def get_azure_client():
    global _AZURE_CLIENT
    if _AZURE_CLIENT is None:
        _AZURE_CLIENT = AzureOpenAI(
            api_key=os.getenv("AZURE_API_KEY"),
            api_version=os.getenv("AZURE_API_VERSION"),
            azure_endpoint=os.getenv("AZURE_ENDPOINT")
        )
    return _AZURE_CLIENT

def get_rag_components():
    global _RAG_COMPONENTS
    
    if _RAG_COMPONENTS is None:
        print("ðŸ”„ [System] Loading RAG system...")
        
        config_path = CONFIGS_DIR / "config.json"
        config = bu.load_config(str(config_path))

        embeddings_dir = DATA_DIR / "embeddings"
        
        print(f"Loading embeddings from: {embeddings_dir}")
        embeddings_data = ru.load_embeddings(str(embeddings_dir))
        
        if not embeddings_data:
            print(f"WARNING: No embeddings found in {embeddings_dir}")
            return None

        model_name = config["embeddings"]["model_name"]
        retrieval_model = ru.load_model(model_name)

        _RAG_COMPONENTS = {
            "config": config,
            "embeddings": embeddings_data["embeddings"],
            "contents": embeddings_data["segment_contents"],
            "sources": embeddings_data.get("segment_sources", []),
            "retrieval_model": retrieval_model
        }
        print("âœ… [System] RAG system loaded.")
        
    return _RAG_COMPONENTS

def generate_answer_stream(query: str, manual_context: str = None, temperature: float = 0):
    client = get_azure_client()
    rag = get_rag_components() 

    final_context = manual_context or ""

    if rag:
        top_k = rag["config"]["retrieve"]["top_k"]
        top_segments, top_similarities, top_sources = ru.search_query(
            query=query,
            corpus_embeddings=rag["embeddings"],
            retrieval_model=rag["retrieval_model"],
            segment_contents=rag["contents"],
            segment_sources=rag["sources"],
            top_k=top_k
        )

        context_list = []
        for txt, src in zip(top_segments, top_sources):
            context_list.append(f"Fonte: {src}\nConteÃºdo: {txt}")
        
        retrieved_context_str = "\n\n---\n\n".join(context_list)
        
        if final_context:
            final_context += f"\n\nInformaÃ§Ã£o Recuperada:\n{retrieved_context_str}"
        else:
            final_context = retrieved_context_str
    else:
        if not final_context:
            final_context = "No context available (RAG not loaded)."

    deployment_name = os.getenv("AZURE_DEPLOYMENT_NAME", "gpt-4")
    
    system_prompt = (
        "VocÃª Ã© o Assistente Virtual do Projeto PROSEI-IA (MÃ³dulo FiscalizaÃ§Ã£o).\n"
        "Baseie-se EXCLUSIVAMENTE no Contexto fornecido.\n"
        "Mantenha tom tÃ©cnico e profissional em PortuguÃªs."
    )

    user_prompt = f"Contexto:\n{final_context}\n\nPergunta:\n{query}\n"

    try:
        response = client.chat.completions.create(
            model=deployment_name, 
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            temperature=temperature,
            stream=True
        )

        for chunk in response:
            if chunk.choices and chunk.choices[0].delta.content:
                yield chunk.choices[0].delta.content

    except Exception as e:
        yield f"Azure Error: {str(e)}"

async def stream_generator(generator: Generator[str, None, None]):
    loop = asyncio.get_running_loop()
    for token in generator:
        yield token
        await asyncio.sleep(0)
